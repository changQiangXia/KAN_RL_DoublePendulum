"""
ä¸“å®¶æ•°æ®ç”Ÿæˆè„šæœ¬ (1_generate_expert.py)
=========================================
ç”Ÿæˆç”¨äºè¡Œä¸ºå…‹éš† (BC) è®­ç»ƒçš„ä¸“å®¶è½¨è¿¹

æ”¯æŒçš„ä¸“å®¶ç­–ç•¥:
1. random: éšæœºç­–ç•¥ (ç”¨äºæµ‹è¯•)
2. heuristic: å¯å‘å¼ç­–ç•¥ (åŸºäºèƒ½é‡å’Œè§’åº¦)
3. mlp_ppo: é¢„è®­ç»ƒçš„ MLP-PPO æ¨¡å‹ (éœ€å…ˆè®­ç»ƒ)

è¾“å‡ºæ ¼å¼:
- data/expert_trajectories.pt
  {
    'states': torch.Tensor (N, 6),
    'actions': torch.Tensor (N, 1),
    'rewards': torch.Tensor (N,),
    'episode_lengths': List[int],
  }

ä½¿ç”¨æ–¹æ³•:
  python scripts/1_generate_expert.py --algorithm heuristic --n_trajectories 1000
"""

import os
import sys
import argparse
import yaml
import torch
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from envs.wrapper import make_acrobot_env
from utils.experts import RandomExpert as ImportedRandomExpert, HeuristicExpert as ImportedHeuristicExpert


# ä¿ç•™æœ¬åœ°å®šä¹‰ä»¥ä¿æŒå‘åå…¼å®¹
class RandomExpert:
    """éšæœºä¸“å®¶ç­–ç•¥ (ç”¨äºæµ‹è¯• BC æµç¨‹)"""
    
    def __init__(self, action_dim: int = 1):
        self.action_dim = action_dim
    
    def get_action(self, state: np.ndarray) -> np.ndarray:
        """è¿”å›éšæœºåŠ¨ä½œ [-1, 1]"""
        return np.random.uniform(-1, 1, size=(self.action_dim,))
    
    def eval(self):
        pass
    
    def __call__(self, state):
        """ä½¿å…¶å¯è°ƒç”¨"""
        return self.get_action(state)


class HeuristicExpert:
    """
    å¯å‘å¼ä¸“å®¶ç­–ç•¥ (åŸºäºç‰©ç†ç›´è§‰)
    
    ç­–ç•¥é€»è¾‘:
    - å¦‚æœä¸‹æ‘†è§’åº¦å¤§ï¼Œæ–½åŠ æ‰­çŸ©ä½¿å…¶å›æ­£
    - è€ƒè™‘è§’é€Ÿåº¦è¿›è¡Œé˜»å°¼æ§åˆ¶
    - ç±»ä¼¼ä¸€ä¸ªç®€å•çš„ PD æ§åˆ¶å™¨
    
    çŠ¶æ€: [cos1, sin1, cos2, sin2, dot1, dot2]
    """
    
    def __init__(
        self,
        kp1: float = 2.0,   # ç¬¬ä¸€æ‘†è§’åº¦æ¯”ä¾‹å¢ç›Š
        kd1: float = 0.5,   # ç¬¬ä¸€æ‘†è§’é€Ÿåº¦é˜»å°¼
        kp2: float = 4.0,   # ç¬¬äºŒæ‘†è§’åº¦æ¯”ä¾‹å¢ç›Š
        kd2: float = 0.3,   # ç¬¬äºŒæ‘†è§’é€Ÿåº¦é˜»å°¼
    ):
        self.kp1 = kp1
        self.kd1 = kd1
        self.kp2 = kp2
        self.kd2 = kd2
    
    def get_action(self, state: np.ndarray) -> np.ndarray:
        """
        æ ¹æ®çŠ¶æ€è®¡ç®—åŠ¨ä½œ
        
        ç›®æ ‡: å°†åŒæ‘†æ‘†åŠ¨åˆ°ç›´ç«‹ä½ç½® (cos1â‰ˆ1, sin1â‰ˆ0, cos2â‰ˆ1, sin2â‰ˆ0)
        """
        cos1, sin1, cos2, sin2, dot1, dot2 = state
        
        # è®¡ç®—è§’åº¦ (ä»ä¸‰è§’å‡½æ•°æ¢å¤)
        theta1 = np.arctan2(sin1, cos1)
        theta2 = np.arctan2(sin2, cos2)
        
        # PD æ§åˆ¶: ç›®æ ‡æ˜¯ theta1=0, theta2=0 (ç›´ç«‹)
        # åŠ¨ä½œ = -kp * theta - kd * dot
        action = (
            -self.kp1 * theta1 - self.kd1 * dot1
            - self.kp2 * theta2 - self.kd2 * dot2
        )
        
        # é™åˆ¶åˆ° [-1, 1]
        action = np.clip(action, -1.0, 1.0)
        
        return np.array([action], dtype=np.float32)
    
    def eval(self):
        pass
    
    def __call__(self, state):
        """ä½¿å…¶å¯è°ƒç”¨"""
        return self.get_action(state)


class MLPPPOExpert:
    """
    é¢„è®­ç»ƒ MLP-PPO ä¸“å®¶ (å ä½å®ç°)
    
    éœ€è¦å…ˆè®­ç»ƒä¸€ä¸ª MLP-PPO æ¨¡å‹ï¼Œç„¶ååŠ è½½
    """
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        # TODO: åŠ è½½ MLP-PPO æ¨¡å‹
        # self.model = load_model(model_path)
        raise NotImplementedError(
            "MLP-PPO ä¸“å®¶éœ€è¦é¢„è®­ç»ƒæ¨¡å‹ã€‚"
            "è¯·å…ˆè®­ç»ƒ MLP-PPO æˆ–é€‰æ‹©å…¶ä»–ä¸“å®¶ç±»å‹ã€‚"
        )
    
    def get_action(self, state: np.ndarray) -> np.ndarray:
        # with torch.no_grad():
        #     action = self.model(state)
        pass
    
    def eval(self):
        pass


def collect_trajectories(
    env,
    expert,
    n_trajectories: int = 1000,
    max_steps: int = 500,
    render: bool = False,
) -> Dict:
    """
    æ”¶é›†ä¸“å®¶è½¨è¿¹
    
    Args:
        env: ç¯å¢ƒå®ä¾‹
        expert: ä¸“å®¶ç­–ç•¥
        n_trajectories: æ”¶é›†è½¨è¿¹æ•°é‡
        max_steps: æ¯æ¡è½¨è¿¹æœ€å¤§æ­¥æ•°
        render: æ˜¯å¦æ¸²æŸ“ (ä»…é€‚ç”¨äºæœ‰ GUI çš„ç¯å¢ƒ)
    
    Returns:
        {
            'states': np.ndarray (N, 6),
            'actions': np.ndarray (N, 1),
            'rewards': np.ndarray (N,),
            'episode_lengths': List[int],
            'episode_rewards': List[float],
        }
    """
    all_states = []
    all_actions = []
    all_rewards = []
    episode_lengths = []
    episode_rewards = []
    
    pbar = tqdm(total=n_trajectories, desc="æ”¶é›†è½¨è¿¹")
    
    for ep in range(n_trajectories):
        obs, _ = env.reset()
        episode_reward = 0
        
        for step in range(max_steps):
            if render:
                env.render()
            
            # è·å–ä¸“å®¶åŠ¨ä½œ
            action = expert.get_action(obs)
            
            # å­˜å‚¨çŠ¶æ€-åŠ¨ä½œå¯¹
            all_states.append(obs.copy())
            all_actions.append(action.copy())
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, terminated, truncated, info = env.step(action)
            all_rewards.append(reward)
            episode_reward += reward
            
            if terminated or truncated:
                break
        
        episode_lengths.append(step + 1)
        episode_rewards.append(episode_reward)
        
        pbar.update(1)
        pbar.set_postfix({
            'avg_reward': np.mean(episode_rewards[-100:]),
            'avg_length': np.mean(episode_lengths[-100:]),
        })
    
    pbar.close()
    
    return {
        'states': np.array(all_states, dtype=np.float32),
        'actions': np.array(all_actions, dtype=np.float32),
        'rewards': np.array(all_rewards, dtype=np.float32),
        'episode_lengths': episode_lengths,
        'episode_rewards': episode_rewards,
    }


def main():
    parser = argparse.ArgumentParser(description="ç”Ÿæˆä¸“å®¶è½¨è¿¹æ•°æ®")
    parser.add_argument(
        '--config', 
        type=str, 
        default='config.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--algorithm',
        type=str,
        default='heuristic',
        choices=['random', 'heuristic', 'mlp_ppo'],
        help='ä¸“å®¶ç®—æ³•ç±»å‹'
    )
    parser.add_argument(
        '--n_trajectories',
        type=int,
        default=1000,
        help='ç”Ÿæˆè½¨è¿¹æ•°é‡'
    )
    parser.add_argument(
        '--max_steps',
        type=int,
        default=500,
        help='æ¯æ¡è½¨è¿¹æœ€å¤§æ­¥æ•°'
    )
    parser.add_argument(
        '--output',
        type=str,
        default=None,
        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤ä» config è¯»å–)'
    )
    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='éšæœºç§å­'
    )
    parser.add_argument(
        '--render',
        action='store_true',
        help='æ˜¯å¦æ¸²æŸ“ç¯å¢ƒ (æ…¢)'
    )
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    if os.path.exists(args.config):
        with open(args.config, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    else:
        print(f"è­¦å‘Š: é…ç½®æ–‡ä»¶ {args.config} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        config = {}
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = args.output or config.get('expert', {}).get('save_path', 'data/expert_trajectories.pt')
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # åˆ›å»ºç¯å¢ƒ
    print(f"åˆ›å»ºç¯å¢ƒ: Acrobot-v1 (è¿ç»­æ§åˆ¶)")
    env = make_acrobot_env(continuous=True, seed=args.seed)
    
    # åˆ›å»ºä¸“å®¶
    print(f"åˆ›å»ºä¸“å®¶ç­–ç•¥: {args.algorithm}")
    if args.algorithm == 'random':
        expert = RandomExpert(action_dim=1)
    elif args.algorithm == 'heuristic':
        expert = HeuristicExpert()
    elif args.algorithm == 'mlp_ppo':
        model_path = config.get('expert', {}).get('model_path', 'checkpoints/expert_mlp_ppo.pt')
        expert = MLPPPOExpert(model_path)
    else:
        raise ValueError(f"æœªçŸ¥çš„ä¸“å®¶ç®—æ³•: {args.algorithm}")
    
    expert.eval()
    
    # æ”¶é›†è½¨è¿¹
    print(f"\nå¼€å§‹æ”¶é›† {args.n_trajectories} æ¡è½¨è¿¹...")
    print(f"æ¯æ¡è½¨è¿¹æœ€å¤§æ­¥æ•°: {args.max_steps}")
    print("-" * 60)
    
    data = collect_trajectories(
        env=env,
        expert=expert,
        n_trajectories=args.n_trajectories,
        max_steps=args.max_steps,
        render=args.render,
    )
    
    env.close()
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 60)
    print("æ•°æ®æ”¶é›†å®Œæˆ!")
    print("=" * 60)
    print(f"æ€»æ ·æœ¬æ•°: {len(data['states'])}")
    print(f"å¹³å‡å›åˆé•¿åº¦: {np.mean(data['episode_lengths']):.1f} Â± {np.std(data['episode_lengths']):.1f}")
    print(f"å¹³å‡å›åˆå¥–åŠ±: {np.mean(data['episode_rewards']):.1f} Â± {np.std(data['episode_rewards']):.1f}")
    print(f"åŠ¨ä½œèŒƒå›´: [{data['actions'].min():.3f}, {data['actions'].max():.3f}]")
    print(f"å¥–åŠ±èŒƒå›´: [{data['rewards'].min():.3f}, {data['rewards'].max():.3f}]")
    
    # ä¿å­˜æ•°æ®
    data_to_save = {
        'states': torch.FloatTensor(data['states']),
        'actions': torch.FloatTensor(data['actions']),
        'rewards': torch.FloatTensor(data['rewards']),
        'episode_lengths': data['episode_lengths'],
        'episode_rewards': data['episode_rewards'],
        'algorithm': args.algorithm,
        'config': config,
    }
    
    torch.save(data_to_save, output_path)
    print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜: {output_path}")
    print(f"   æ–‡ä»¶å¤§å°: {os.path.getsize(output_path) / 1024**2:.2f} MB")
    print("=" * 60)


if __name__ == "__main__":
    main()
