"""
è¡Œä¸ºå…‹éš† (Behavioral Cloning) Agent
=====================================
åŸºäº KAN ç½‘ç»œçš„æ¨¡ä»¿å­¦ä¹ å®ç°

è®­ç»ƒæµç¨‹:
1. åŠ è½½ä¸“å®¶è½¨è¿¹ (state-action pairs)
2. ä½¿ç”¨ MSE Loss è®­ç»ƒ KAN ç­–ç•¥
3. L1 æ­£åˆ™åŒ–é¼“åŠ±ç½‘ç»œç¨€ç– (ä¾¿äºç¬¦å·æå–)
4. å®šæœŸæ›´æ–° B-spline ç½‘æ ¼
5. æ—©åœå’Œæ¨¡å‹ä¿å­˜

å…³é”®ç‰¹æ€§:
- æ¢¯åº¦è£å‰ªé˜²æ­¢ KAN ä¸ç¨³å®š
- éªŒè¯é›†ç›‘æ§é˜²æ­¢è¿‡æ‹Ÿåˆ
- ç½‘æ ¼æ›´æ–°é¢‘ç‡å¯é…ç½®
- ç¨€ç–åŒ–ç»Ÿè®¡å®æ—¶ç›‘æ§
"""

import os
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
from typing import Dict, List, Tuple, Optional, Callable
from pathlib import Path
import time

from models.kan_policy import KANPolicy


class ExpertDataset(Dataset):
    """
    ä¸“å®¶è½¨è¿¹æ•°æ®é›†
    
    æ•°æ®æ ¼å¼:
    - states: (N, 6) çŠ¶æ€å‘é‡
    - actions: (N, 1) è¿ç»­åŠ¨ä½œ
    """
    
    def __init__(self, data_path: str):
        """
        Args:
            data_path: ä¸“å®¶æ•°æ®æ–‡ä»¶è·¯å¾„ (.npy æˆ– .pt)
        """
        super().__init__()
        
        # åŠ è½½æ•°æ®
        if data_path.endswith('.pt') or data_path.endswith('.pth'):
            data = torch.load(data_path)
            self.states = data['states'].float()
            self.actions = data['actions'].float()
        elif data_path.endswith('.npy'):
            data = np.load(data_path, allow_pickle=True).item()
            self.states = torch.FloatTensor(data['states'])
            self.actions = torch.FloatTensor(data['actions'])
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {data_path}")
        
        # ç¡®ä¿åŠ¨ä½œæ˜¯ 2D
        if self.actions.dim() == 1:
            self.actions = self.actions.unsqueeze(-1)
        
        assert self.states.shape[0] == self.actions.shape[0], "çŠ¶æ€å’ŒåŠ¨ä½œæ•°é‡ä¸åŒ¹é…"
        
        print(f"[ExpertDataset] åŠ è½½ {len(self)} æ¡ä¸“å®¶è½¨è¿¹")
        print(f"  - states shape: {self.states.shape}")
        print(f"  - actions shape: {self.actions.shape}")
        print(f"  - actions range: [{self.actions.min():.3f}, {self.actions.max():.3f}]")
    
    def __len__(self) -> int:
        return self.states.shape[0]
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        return self.states[idx], self.actions[idx]


class BCAgent:
    """
    è¡Œä¸ºå…‹éš† Agent
    
    ä½¿ç”¨ KAN ç½‘ç»œä»ä¸“å®¶æ•°æ®ä¸­å­¦ä¹ ç­–ç•¥
    """
    
    def __init__(
        self,
        policy: Optional[KANPolicy] = None,
        config: Optional[Dict] = None,
        config_path: Optional[str] = None,
        device: Optional[str] = None,
    ):
        """
        Args:
            policy: KAN ç­–ç•¥ç½‘ç»œ (è‹¥ None åˆ™æ ¹æ®é…ç½®åˆ›å»º)
            config: é…ç½®å­—å…¸ (è‹¥ None åˆ™ä» config_path åŠ è½½)
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            device: è®¡ç®—è®¾å¤‡ ('cuda', 'cpu', æˆ– None è‡ªåŠ¨æ£€æµ‹)
        """
        # åŠ è½½é…ç½®
        if config is not None:
            self.config = config
        elif config_path is not None:
            with open(config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
        else:
            # é»˜è®¤é…ç½®
            self.config = {
                'bc': {
                    'batch_size': 128,
                    'lr': 1e-3,
                    'l1_penalty': 1e-4,
                    'grad_clip': 1.0,
                    'val_split': 0.1,
                    'early_stop_patience': 20,
                },
                'model': {
                    'layers': [6, 8, 1],
                    'grid_size': 5,
                    'spline_order': 3,
                },
            }
        
        bc_config = self.config.get('bc', {})
        model_config = self.config.get('model', {})
        
        # è®¾å¤‡
        if device is None:
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        else:
            self.device = device
        print(f"[BCAgent] ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # ç­–ç•¥ç½‘ç»œ
        if policy is not None:
            self.policy = policy.to(self.device)
        else:
            layers = model_config.get('layers', [6, 8, 1])
            self.policy = KANPolicy(
                input_dim=int(layers[0]),
                hidden_dim=int(layers[1]),
                output_dim=int(layers[2]),
                grid_size=int(model_config.get('grid_size', 5)),
                spline_order=int(model_config.get('spline_order', 3)),
            ).to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.lr = float(bc_config.get('lr', 1e-3))
        self.optimizer = optim.Adam(self.policy.parameters(), lr=self.lr)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ (å¯é€‰)
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=10, verbose=True
        )
        
        # è®­ç»ƒå‚æ•°
        self.batch_size = int(bc_config.get('batch_size', 128))
        self.l1_penalty = float(bc_config.get('l1_penalty', 1e-4))
        self.grad_clip = float(bc_config.get('grad_clip', 1.0))
        self.val_split = float(bc_config.get('val_split', 0.1))
        self.early_stop_patience = int(bc_config.get('early_stop_patience', 20))
        
        # ç½‘æ ¼æ›´æ–°å‚æ•°
        self.grid_update_freq = int(model_config.get('grid_update_freq', 10))
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        self.train_losses = []
        self.val_losses = []
        
    def create_dataloaders(self, data_path: str) -> Tuple[DataLoader, DataLoader]:
        """
        åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯ DataLoader
        
        Args:
            data_path: ä¸“å®¶æ•°æ®è·¯å¾„
        Returns:
            train_loader, val_loader
        """
        dataset = ExpertDataset(data_path)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        val_size = int(len(dataset) * self.val_split)
        train_size = len(dataset) - val_size
        
        train_dataset, val_dataset = random_split(
            dataset, 
            [train_size, val_size],
            generator=torch.Generator().manual_seed(42)
        )
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=0,  # Windows ä¸‹å»ºè®®è®¾ä¸º 0
            pin_memory=True if self.device == 'cuda' else False,
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=0,
            pin_memory=True if self.device == 'cuda' else False,
        )
        
        print(f"[BCAgent] æ•°æ®é›†åˆ’åˆ†: è®­ç»ƒé›† {train_size}, éªŒè¯é›† {val_size}")
        
        return train_loader, val_loader
    
    def compute_loss(
        self,
        pred_actions: torch.Tensor,
        true_actions: torch.Tensor,
    ) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        è®¡ç®—æŸå¤± = MSE + L1 æ­£åˆ™åŒ–
        
        Returns:
            total_loss, loss_info
        """
        # MSE Loss
        mse_loss = F.mse_loss(pred_actions, true_actions)
        
        # L1 æ­£åˆ™åŒ– (ç¨€ç–åŒ–æƒ©ç½š)
        if self.l1_penalty > 0:
            l1_loss = self.policy.regularization_loss(self.l1_penalty)
        else:
            l1_loss = torch.tensor(0.0, device=self.device)
        
        # æ€»æŸå¤±
        total_loss = mse_loss + l1_loss
        
        loss_info = {
            'total': total_loss.item(),
            'mse': mse_loss.item(),
            'l1': l1_loss.item(),
        }
        
        return total_loss, loss_info
    
    def train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """
        è®­ç»ƒä¸€ä¸ª epoch
        
        Returns:
            å¹³å‡æŸå¤±å­—å…¸
        """
        self.policy.train()
        total_losses = {'total': 0.0, 'mse': 0.0, 'l1': 0.0}
        n_batches = 0
        
        for states, actions in train_loader:
            states = states.to(self.device)
            actions = actions.to(self.device)
            
            # å‰å‘ä¼ æ’­
            pred_actions = self.policy(states)
            
            # è®¡ç®—æŸå¤±
            loss, loss_info = self.compute_loss(pred_actions, actions)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª (å…³é”®ï¼é˜²æ­¢ KAN æ¢¯åº¦çˆ†ç‚¸)
            if self.grad_clip > 0:
                torch.nn.utils.clip_grad_norm_(
                    self.policy.parameters(),
                    self.grad_clip
                )
            
            self.optimizer.step()
            
            # ç´¯åŠ æŸå¤±
            for key in total_losses:
                total_losses[key] += loss_info[key]
            n_batches += 1
        
        # å¹³å‡æŸå¤±
        avg_losses = {key: val / n_batches for key, val in total_losses.items()}
        
        return avg_losses
    
    def validate(self, val_loader: DataLoader) -> Dict[str, float]:
        """
        éªŒè¯
        
        Returns:
            å¹³å‡æŸå¤±å­—å…¸
        """
        self.policy.eval()
        total_losses = {'total': 0.0, 'mse': 0.0, 'l1': 0.0}
        n_batches = 0
        
        with torch.no_grad():
            for states, actions in val_loader:
                states = states.to(self.device)
                actions = actions.to(self.device)
                
                pred_actions = self.policy(states)
                _, loss_info = self.compute_loss(pred_actions, actions)
                
                for key in total_losses:
                    total_losses[key] += loss_info[key]
                n_batches += 1
        
        avg_losses = {key: val / n_batches for key, val in total_losses.items()}
        
        return avg_losses
    
    def train(
        self,
        data_path: str,
        epochs: int = 200,
        save_path: Optional[str] = None,
        log_interval: int = 10,
        verbose: bool = True,
    ) -> Dict[str, List[float]]:
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        
        Args:
            data_path: ä¸“å®¶æ•°æ®è·¯å¾„
            epochs: è®­ç»ƒè½®æ•°
            save_path: æœ€ä½³æ¨¡å‹ä¿å­˜è·¯å¾„
            log_interval: æ—¥å¿—æ‰“å°é—´éš”
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—
        
        Returns:
            è®­ç»ƒå†å²è®°å½•
        """
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader = self.create_dataloaders(data_path)
        
        print(f"\n{'='*60}")
        print("å¼€å§‹è¡Œä¸ºå…‹éš†è®­ç»ƒ")
        print(f"{'='*60}")
        print(f"Epochs: {epochs}")
        print(f"Batch size: {self.batch_size}")
        print(f"Learning rate: {self.lr}")
        print(f"L1 penalty: {self.l1_penalty}")
        print(f"Grad clip: {self.grad_clip}")
        print(f"Grid update freq: {self.grid_update_freq}")
        print(f"{'='*60}\n")
        
        start_time = time.time()
        
        for epoch in range(epochs):
            self.current_epoch = epoch
            
            # è®­ç»ƒ
            train_losses = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_losses = self.validate(val_loader)
            
            # è®°å½•å†å²
            self.train_losses.append(train_losses['total'])
            self.val_losses.append(val_losses['total'])
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(val_losses['total'])
            
            # ç½‘æ ¼æ›´æ–°
            if (epoch + 1) % self.grid_update_freq == 0:
                print(f"[Epoch {epoch+1}] æ›´æ–° B-spline ç½‘æ ¼...")
                # ä»éªŒè¯é›†é‡‡æ ·ä¸€äº›çŠ¶æ€ç”¨äºç½‘æ ¼æ›´æ–°
                sample_states = []
                for states, _ in val_loader:
                    sample_states.append(states)
                    if len(sample_states) * states.shape[0] >= 2048:
                        break
                sample_states = torch.cat(sample_states, dim=0)[:2048].to(self.device)
                self.policy.update_grids(sample_states, sample_rate=1.0)
            
            # æ—©åœæ£€æŸ¥
            if val_losses['total'] < self.best_val_loss:
                self.best_val_loss = val_losses['total']
                self.patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if save_path is not None:
                    self.save(save_path)
                    if verbose and (epoch + 1) % log_interval == 0:
                        print(f"[Epoch {epoch+1}] ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (val_loss={val_losses['total']:.6f})")
            else:
                self.patience_counter += 1
                if self.patience_counter >= self.early_stop_patience:
                    print(f"\n[Epoch {epoch+1}] â¹ï¸ æ—©åœè§¦å‘ ( patience={self.early_stop_patience} )")
                    break
            
            # æ‰“å°æ—¥å¿—
            if verbose and (epoch + 1) % log_interval == 0:
                elapsed = time.time() - start_time
                print(
                    f"[Epoch {epoch+1:3d}/{epochs}] "
                    f"train_loss={train_losses['total']:.6f} "
                    f"(mse={train_losses['mse']:.6f}, l1={train_losses['l1']:.6f}) | "
                    f"val_loss={val_losses['total']:.6f} "
                    f"| è€—æ—¶: {elapsed:.1f}s"
                )
                
                # æ‰“å°ç¨€ç–åŒ–ä¿¡æ¯ (æ¯ 20 ä¸ª epoch)
                if (epoch + 1) % (log_interval * 2) == 0:
                    self.policy.print_sparsity(threshold=0.01)
        
        total_time = time.time() - start_time
        print(f"\n{'='*60}")
        print(f"è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {total_time:.1f}s")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f}")
        print(f"{'='*60}\n")
        
        return {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
        }
    
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        torch.save({
            'policy_state_dict': self.policy.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epoch': self.current_epoch,
            'best_val_loss': self.best_val_loss,
            'config': self.config,
        }, path)
    
    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.device)
        self.policy.load_state_dict(checkpoint['policy_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.current_epoch = checkpoint.get('epoch', 0)
        self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))
        print(f"[BCAgent] åŠ è½½æ¨¡å‹: {path} (epoch={self.current_epoch})")
    
    def evaluate(self, env, n_episodes: int = 10) -> Dict[str, float]:
        """
        åœ¨ç¯å¢ƒä¸­è¯„ä¼°ç­–ç•¥
        
        Args:
            env: ç¯å¢ƒå®ä¾‹
            n_episodes: è¯„ä¼°å›åˆæ•°
        Returns:
            è¯„ä¼°ç»Ÿè®¡ä¿¡æ¯
        """
        self.policy.eval()
        episode_rewards = []
        episode_lengths = []
        
        for ep in range(n_episodes):
            obs, _ = env.reset()
            episode_reward = 0
            episode_length = 0
            done = False
            
            while not done:
                action = self.policy.get_action(obs)
                obs, reward, terminated, truncated, _ = env.step(action)
                episode_reward += reward
                episode_length += 1
                done = terminated or truncated
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
        
        return {
            'mean_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'mean_length': np.mean(episode_lengths),
            'std_length': np.std(episode_lengths),
        }


# å¯¼å…¥ F ç”¨äºæŸå¤±è®¡ç®—
import torch.nn.functional as F


def test_bc_agent():
    """æµ‹è¯• BC Agent"""
    print("=" * 60)
    print("æµ‹è¯• BC Agent")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡æ‹Ÿä¸“å®¶æ•°æ®
    print("\n[Step 1] åˆ›å»ºæ¨¡æ‹Ÿä¸“å®¶æ•°æ®")
    n_samples = 1000
    states = torch.randn(n_samples, 6)
    actions = torch.tanh(torch.randn(n_samples, 1))  # èŒƒå›´ [-1, 1]
    
    # æ·»åŠ ä¸€äº›æ¨¡å¼ä½¿ä»»åŠ¡å¯å­¦ä¹ 
    # ç®€å•è§„åˆ™: action = -0.5 * cos1 + 0.3 * sin2
    actions = (-0.5 * states[:, 0:1] + 0.3 * states[:, 3:4]).clamp(-1, 1)
    
    # ä¿å­˜æ¨¡æ‹Ÿæ•°æ®
    os.makedirs('data', exist_ok=True)
    mock_data_path = 'data/mock_expert.pt'
    torch.save({'states': states, 'actions': actions}, mock_data_path)
    print(f"æ¨¡æ‹Ÿæ•°æ®å·²ä¿å­˜: {mock_data_path}")
    print(f"  - states: {states.shape}, actions: {actions.shape}")
    
    # åˆ›å»º Agent
    print("\n[Step 2] åˆ›å»º BC Agent")
    config = {
        'bc': {
            'batch_size': 64,  # å°æ‰¹æ¬¡æµ‹è¯•
            'lr': 1e-3,
            'l1_penalty': 1e-4,
            'grad_clip': 1.0,
            'val_split': 0.2,
            'early_stop_patience': 10,
        },
        'model': {
            'layers': [6, 8, 1],
            'grid_size': 5,
            'spline_order': 3,
            'grid_update_freq': 5,
        },
    }
    
    agent = BCAgent(config=config, device='cpu')
    print(f"Policy å‚æ•°æ•°é‡: {sum(p.numel() for p in agent.policy.parameters())}")
    
    # è®­ç»ƒ (çŸ­è½®æ•°æµ‹è¯•)
    print("\n[Step 3] è®­ç»ƒ")
    history = agent.train(
        data_path=mock_data_path,
        epochs=20,
        save_path='checkpoints/test_bc_model.pt',
        log_interval=5,
    )
    
    # éªŒè¯
    print("\n[Step 4] éªŒè¯å­¦ä¹ æ•ˆæœ")
    agent.policy.eval()
    with torch.no_grad():
        test_states = states[:10]
        pred_actions = agent.policy(test_states)
        true_actions = actions[:10]
        mse = F.mse_loss(pred_actions, true_actions).item()
    
    print(f"æµ‹è¯•é›† MSE: {mse:.6f}")
    print(f"é¢„æµ‹åŠ¨ä½œèŒƒå›´: [{pred_actions.min():.3f}, {pred_actions.max():.3f}]")
    print(f"çœŸå®åŠ¨ä½œèŒƒå›´: [{true_actions.min():.3f}, {true_actions.max():.3f}]")
    
    # æ‰“å°æœ€ç»ˆç¨€ç–åŒ–ä¿¡æ¯
    print("\n[Step 5] æœ€ç»ˆç¨€ç–åŒ–ç»Ÿè®¡")
    agent.policy.print_sparsity(threshold=0.01)
    
    print("\n" + "=" * 60)
    print("âœ… BC Agent æµ‹è¯•é€šè¿‡ï¼")
    print("=" * 60)
    
    return agent


if __name__ == "__main__":
    test_bc_agent()
